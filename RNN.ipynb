{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "In this Notebook I will start with the very Basics of RNN's and Build all the way to latest deep learning architectures to solve NLP problems. It will cover the Following:\n",
    "* Simple RNN's\n",
    "* Word Embeddings : Definition and How to get them\n",
    "* LSTM's\n",
    "* GRU's\n",
    "* BI-Directional RNN's\n",
    "* Encoder-Decoder Models (Seq2Seq Models)\n",
    "* Attention Models\n",
    "* Transformers - Attention is all you need\n",
    "* BERT\n",
    "\n",
    "I will divide every Topic into four subsections:\n",
    "* Basic Overview\n",
    "* In-Depth Understanding : In this I will attach links of articles and videos to learn about the topic in depth\n",
    "* Code-Implementation\n",
    "* Code Explanation\n",
    "\n",
    "This is a comprehensive kernel and if you follow along till the end , I promise you would learn all the techniques completely\n",
    "\n",
    "Note that the aim of this notebook is not to have a High LB score but to present a beginner guide to understand Deep Learning techniques used for NLP. Also after discussing all of these ideas , I will present a starter solution for this competiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU,SimpleRNN\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "# from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('jigsaw-toxic-comment-train.csv')\n",
    "validation = pd.read_csv('validation.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223549, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Doctor Who adlı viki başlığına 12. doctor olar...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Вполне возможно, но я пока не вижу необходимо...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Quindi tu sei uno di quelli   conservativi  , ...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Malesef gerçekleştirilmedi ancak şöyle bir şey...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>:Resim:Seldabagcan.jpg resminde kaynak sorunu ...</td>\n",
       "      <td>tr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            content lang\n",
       "0   0  Doctor Who adlı viki başlığına 12. doctor olar...   tr\n",
       "1   1   Вполне возможно, но я пока не вижу необходимо...   ru\n",
       "2   2  Quindi tu sei uno di quelli   conservativi  , ...   it\n",
       "3   3  Malesef gerçekleştirilmedi ancak şöyle bir şey...   tr\n",
       "4   4  :Resim:Seldabagcan.jpg resminde kaynak sorunu ...   tr"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Este usuario ni siquiera llega al rango de    ...</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Il testo di questa voce pare esser scopiazzato...</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Vale. Sólo expongo mi pasado. Todo tiempo pasa...</td>\n",
       "      <td>es</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bu maddenin alt başlığı olarak  uluslararası i...</td>\n",
       "      <td>tr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Belçika nın şehirlerinin yanında ilçe ve belde...</td>\n",
       "      <td>tr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       comment_text lang  toxic\n",
       "0   0  Este usuario ni siquiera llega al rango de    ...   es      0\n",
       "1   1  Il testo di questa voce pare esser scopiazzato...   it      0\n",
       "2   2  Vale. Sólo expongo mi pasado. Todo tiempo pasa...   es      1\n",
       "3   3  Bu maddenin alt başlığı olarak  uluslararası i...   tr      0\n",
       "4   4  Belçika nın şehirlerinin yanında ilçe ve belde...   tr      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the other columns and approach this problem as a Binary Classification Problem and also we will have our exercise done on a smaller subsection of the dataset(only 12000 data points) to make it easier to train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0\n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12001, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.loc[:12000,:]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check the maximum number of words that can be present in a comment , this will help us in padding later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         43\n",
       "1         17\n",
       "2         42\n",
       "3        113\n",
       "4         13\n",
       "        ... \n",
       "11996     51\n",
       "11997      4\n",
       "11998    132\n",
       "11999     10\n",
       "12000    195\n",
       "Name: comment_text, Length: 12001, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text'].apply(lambda x:len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1403"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text'].apply(lambda x:len(str(x).split())).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function for getting auc score for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(predictions,target):\n",
    "    '''\n",
    "    This methods returns the AUC Score when given the Predictions\n",
    "    and Labels\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.comment_text.values, train.toxic.values, \n",
    "                                                  stratify=train.toxic.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)\n",
    "\n",
    "## here train.comment_text.values is our independent info i.e and train.toxic.values  is dependent variable or target variable.\n",
    "## also, we are performing stratify split on target variable as we have imbalance data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RNN\n",
    "\n",
    "## Basic Overview\n",
    "\n",
    "What is a RNN?\n",
    "\n",
    "Recurrent Neural Network(RNN) are a type of Neural Network where the output from previous step are fed as input to the current step. In traditional neural networks, all the inputs and outputs are independent of each other, but in cases like when it is required to predict the next word of a sentence, the previous words are required and hence there is a need to remember the previous words. Thus RNN came into existence, which solved this issue with the help of a Hidden Layer.\n",
    "\n",
    "Why RNN's? \n",
    "- Sequential data (e.g., time series, natural language)\n",
    "    - No need for frame-based processing or parallel computation\n",
    "    \n",
    "How does an RNN work?\n",
    "\n",
    "1. Each word in the sentence gets passed forward through the network along with all previous words. This forms a \"memory\" that contains information about all previously seen words.\n",
    "2. Each word in the sentence gets passed on to the next layer along with information about all other words that have come before it. This allows the network to capture dependencies between words.\n",
    "3. Each word in the sentence is treated as a unique \"token\" that can be passed from one timestep to another. This allows the model to keep track of information about each token.\n",
    "\n",
    "https://www.quora.com/Why-do-we-use-an-RNN-instead-of-a-simple-neural-network\n",
    "\n",
    "## In-Depth Understanding\n",
    "\n",
    "* https://medium.com/mindorks/understanding-the-recurrent-neural-network-44d593f112a2\n",
    "* https://www.youtube.com/watch?v=2E65LDnM2cA&list=PL1F3ABbhcqa3BBWo170U4Ev2wfsF7FN8l\n",
    "* https://www.d2l.ai/chapter_recurrent-neural-networks/rnn.html\n",
    "\n",
    "## Code Implementation\n",
    "\n",
    "So first I will implement the and then I will explain the code step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x189f03981f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "token\n",
    "\n",
    "## Here, we are creating tokenier onject using Keras library for text preprocessing. \n",
    "## And using None for num_words, so that it will consider all the words present in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9600,), (2401,), 12001)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape, xvalid.shape, len(list(xtrain) + list(xvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 1500 \n",
    "#This variable max_len is set to 1500, which suggests that the sequences generated by the tokenizer \n",
    "#should be truncated or padded to have a maximum length of 1500 tokens.\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "# The fit_on_texts method is used to update the internal vocabulary based on the text data provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "## The texts_to_sequences method is used to convert each text in the input data (xtrain and xvalid) \n",
    "## into a sequence of integers, where each integer corresponds to the index of a word in the tokenizer's internal vocabulary.\n",
    "## This step essentially transforms the raw text data into sequences of numerical values,\n",
    "# which can be used as input to neural networks or other machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero pad the sequences \n",
    "xtrain_pad = tf.keras.utils.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = tf.keras.utils.pad_sequences(xvalid_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'to': 2,\n",
       " 'of': 3,\n",
       " 'and': 4,\n",
       " 'a': 5,\n",
       " 'you': 6,\n",
       " 'i': 7,\n",
       " 'is': 8,\n",
       " 'that': 9,\n",
       " 'in': 10,\n",
       " 'it': 11,\n",
       " 'for': 12,\n",
       " 'this': 13,\n",
       " 'not': 14,\n",
       " 'on': 15,\n",
       " 'be': 16,\n",
       " 'as': 17,\n",
       " 'are': 18,\n",
       " 'have': 19,\n",
       " 'with': 20,\n",
       " 'your': 21,\n",
       " 'if': 22,\n",
       " 'was': 23,\n",
       " 'article': 24,\n",
       " 'or': 25,\n",
       " 'but': 26,\n",
       " 'page': 27,\n",
       " 'my': 28,\n",
       " 'an': 29,\n",
       " 'wikipedia': 30,\n",
       " 'by': 31,\n",
       " 'from': 32,\n",
       " 'do': 33,\n",
       " 'at': 34,\n",
       " 'about': 35,\n",
       " 'me': 36,\n",
       " 'so': 37,\n",
       " 'talk': 38,\n",
       " 'can': 39,\n",
       " 'what': 40,\n",
       " 'there': 41,\n",
       " 'all': 42,\n",
       " 'has': 43,\n",
       " 'no': 44,\n",
       " 'will': 45,\n",
       " 'one': 46,\n",
       " 'would': 47,\n",
       " 'like': 48,\n",
       " 'please': 49,\n",
       " 'he': 50,\n",
       " 'just': 51,\n",
       " 'they': 52,\n",
       " 'any': 53,\n",
       " 'which': 54,\n",
       " 'been': 55,\n",
       " 'more': 56,\n",
       " 'other': 57,\n",
       " 'we': 58,\n",
       " \"don't\": 59,\n",
       " 'his': 60,\n",
       " 'should': 61,\n",
       " 'some': 62,\n",
       " 'here': 63,\n",
       " 'see': 64,\n",
       " 'who': 65,\n",
       " 'also': 66,\n",
       " 'because': 67,\n",
       " 'know': 68,\n",
       " 'am': 69,\n",
       " 'think': 70,\n",
       " \"i'm\": 71,\n",
       " 'edit': 72,\n",
       " 'how': 73,\n",
       " 'up': 74,\n",
       " 'why': 75,\n",
       " 'out': 76,\n",
       " \"it's\": 77,\n",
       " 'then': 78,\n",
       " 'people': 79,\n",
       " 'use': 80,\n",
       " 'only': 81,\n",
       " 'were': 82,\n",
       " 'time': 83,\n",
       " 'when': 84,\n",
       " 'articles': 85,\n",
       " 'being': 86,\n",
       " 'may': 87,\n",
       " 'did': 88,\n",
       " 'thanks': 89,\n",
       " 'user': 90,\n",
       " 'now': 91,\n",
       " 'their': 92,\n",
       " 'than': 93,\n",
       " 'even': 94,\n",
       " 'them': 95,\n",
       " 'get': 96,\n",
       " 'fuck': 97,\n",
       " 'fish': 98,\n",
       " 'make': 99,\n",
       " 'go': 100,\n",
       " 'good': 101,\n",
       " 'had': 102,\n",
       " 'information': 103,\n",
       " 'could': 104,\n",
       " 'well': 105,\n",
       " 'very': 106,\n",
       " 'its': 107,\n",
       " 'deletion': 108,\n",
       " 'does': 109,\n",
       " 'sources': 110,\n",
       " 'balls': 111,\n",
       " 'wp': 112,\n",
       " 'want': 113,\n",
       " 'such': 114,\n",
       " 'first': 115,\n",
       " 'way': 116,\n",
       " 'name': 117,\n",
       " 'say': 118,\n",
       " 'made': 119,\n",
       " 'new': 120,\n",
       " 'image': 121,\n",
       " 'pages': 122,\n",
       " 'source': 123,\n",
       " 'going': 124,\n",
       " 'these': 125,\n",
       " 'again': 126,\n",
       " 'ass': 127,\n",
       " 'used': 128,\n",
       " 'where': 129,\n",
       " 'really': 130,\n",
       " 'section': 131,\n",
       " 'edits': 132,\n",
       " 'sex': 133,\n",
       " 'need': 134,\n",
       " 'most': 135,\n",
       " 'many': 136,\n",
       " 'help': 137,\n",
       " 'same': 138,\n",
       " 'much': 139,\n",
       " 'thank': 140,\n",
       " 'since': 141,\n",
       " \"i've\": 142,\n",
       " 'editing': 143,\n",
       " 'find': 144,\n",
       " 'after': 145,\n",
       " 'yourself': 146,\n",
       " 'read': 147,\n",
       " 'wiki': 148,\n",
       " 'discussion': 149,\n",
       " 'into': 150,\n",
       " 'right': 151,\n",
       " 'hate': 152,\n",
       " 'those': 153,\n",
       " 'before': 154,\n",
       " 'said': 155,\n",
       " 'fact': 156,\n",
       " 'him': 157,\n",
       " 'two': 158,\n",
       " 'someone': 159,\n",
       " 'work': 160,\n",
       " 'own': 161,\n",
       " 'look': 162,\n",
       " 'point': 163,\n",
       " 'something': 164,\n",
       " 'too': 165,\n",
       " 'deleted': 166,\n",
       " 'take': 167,\n",
       " 'still': 168,\n",
       " 'over': 169,\n",
       " 'add': 170,\n",
       " 'back': 171,\n",
       " 'her': 172,\n",
       " 'link': 173,\n",
       " 'history': 174,\n",
       " 'stop': 175,\n",
       " 'under': 176,\n",
       " 'content': 177,\n",
       " 'utc': 178,\n",
       " 'u': 179,\n",
       " '1': 180,\n",
       " 'blocked': 181,\n",
       " 'never': 182,\n",
       " 'case': 183,\n",
       " 'list': 184,\n",
       " 'added': 185,\n",
       " 'without': 186,\n",
       " \"you're\": 187,\n",
       " 'comment': 188,\n",
       " '2': 189,\n",
       " 'removed': 190,\n",
       " 'place': 191,\n",
       " 'editors': 192,\n",
       " 'might': 193,\n",
       " 'better': 194,\n",
       " 'another': 195,\n",
       " 'however': 196,\n",
       " 'using': 197,\n",
       " 'our': 198,\n",
       " 'note': 199,\n",
       " 'free': 200,\n",
       " 'ask': 201,\n",
       " 'personal': 202,\n",
       " \"doesn't\": 203,\n",
       " 'she': 204,\n",
       " 'done': 205,\n",
       " 'http': 206,\n",
       " 'reason': 207,\n",
       " 'block': 208,\n",
       " 'put': 209,\n",
       " 'anything': 210,\n",
       " \"that's\": 211,\n",
       " 'question': 212,\n",
       " 'thing': 213,\n",
       " 'sure': 214,\n",
       " 'person': 215,\n",
       " 'while': 216,\n",
       " 'things': 217,\n",
       " 'off': 218,\n",
       " 'believe': 219,\n",
       " 'both': 220,\n",
       " 'feel': 221,\n",
       " 'vandalism': 222,\n",
       " 'policy': 223,\n",
       " 'welcome': 224,\n",
       " 'actually': 225,\n",
       " 'part': 226,\n",
       " 'seems': 227,\n",
       " 'us': 228,\n",
       " 'nigger': 229,\n",
       " 'though': 230,\n",
       " 'hope': 231,\n",
       " 'against': 232,\n",
       " 'come': 233,\n",
       " 'problem': 234,\n",
       " 'com': 235,\n",
       " \"didn't\": 236,\n",
       " 'change': 237,\n",
       " 'comments': 238,\n",
       " \"can't\": 239,\n",
       " 'editor': 240,\n",
       " 'world': 241,\n",
       " 'sorry': 242,\n",
       " 'hi': 243,\n",
       " 'life': 244,\n",
       " 'questions': 245,\n",
       " 'post': 246,\n",
       " '3': 247,\n",
       " 'reference': 248,\n",
       " 'different': 249,\n",
       " 'little': 250,\n",
       " 'links': 251,\n",
       " \"i'll\": 252,\n",
       " 'already': 253,\n",
       " 'wrong': 254,\n",
       " 'understand': 255,\n",
       " 'years': 256,\n",
       " 'oh': 257,\n",
       " 'give': 258,\n",
       " 'making': 259,\n",
       " 'others': 260,\n",
       " 'agree': 261,\n",
       " 'nothing': 262,\n",
       " 'keep': 263,\n",
       " 'anyone': 264,\n",
       " 'reliable': 265,\n",
       " 'must': 266,\n",
       " 'english': 267,\n",
       " 'fair': 268,\n",
       " 'copyright': 269,\n",
       " 'mean': 270,\n",
       " 'rather': 271,\n",
       " 'best': 272,\n",
       " 'last': 273,\n",
       " 'issue': 274,\n",
       " 'subject': 275,\n",
       " 'far': 276,\n",
       " 'remove': 277,\n",
       " \"isn't\": 278,\n",
       " 'leave': 279,\n",
       " 'write': 280,\n",
       " 'etc': 281,\n",
       " 'long': 282,\n",
       " 'references': 283,\n",
       " 'tag': 284,\n",
       " 'above': 285,\n",
       " 'few': 286,\n",
       " 'e': 287,\n",
       " 'says': 288,\n",
       " 'trying': 289,\n",
       " 'non': 290,\n",
       " 'day': 291,\n",
       " 'found': 292,\n",
       " 'word': 293,\n",
       " 'doing': 294,\n",
       " 'got': 295,\n",
       " 'try': 296,\n",
       " 'evidence': 297,\n",
       " 'either': 298,\n",
       " 'delete': 299,\n",
       " 'sexsex': 300,\n",
       " 'every': 301,\n",
       " 'yes': 302,\n",
       " 'example': 303,\n",
       " 'probably': 304,\n",
       " 'through': 305,\n",
       " 'speedy': 306,\n",
       " 'simply': 307,\n",
       " 'between': 308,\n",
       " 'least': 309,\n",
       " 'text': 310,\n",
       " 'called': 311,\n",
       " 'encyclopedia': 312,\n",
       " 'great': 313,\n",
       " 'site': 314,\n",
       " 'users': 315,\n",
       " 'having': 316,\n",
       " 'book': 317,\n",
       " 'show': 318,\n",
       " 'based': 319,\n",
       " 'around': 320,\n",
       " 'original': 321,\n",
       " 'yet': 322,\n",
       " 'let': 323,\n",
       " 'request': 324,\n",
       " 'war': 325,\n",
       " 'reverted': 326,\n",
       " 'images': 327,\n",
       " 'opinion': 328,\n",
       " 'else': 329,\n",
       " 'enough': 330,\n",
       " 'perhaps': 331,\n",
       " 'created': 332,\n",
       " 'hello': 333,\n",
       " 'material': 334,\n",
       " 'down': 335,\n",
       " 'www': 336,\n",
       " 'ever': 337,\n",
       " 'account': 338,\n",
       " 'state': 339,\n",
       " 'check': 340,\n",
       " 'version': 341,\n",
       " 'view': 342,\n",
       " 'adding': 343,\n",
       " 'clearly': 344,\n",
       " 'states': 345,\n",
       " 're': 346,\n",
       " 'clear': 347,\n",
       " 'contributions': 348,\n",
       " 'message': 349,\n",
       " 'written': 350,\n",
       " 'bot': 351,\n",
       " 'style': 352,\n",
       " 'continue': 353,\n",
       " 'further': 354,\n",
       " 'consensus': 355,\n",
       " 'bit': 356,\n",
       " 'top': 357,\n",
       " 'needs': 358,\n",
       " 'support': 359,\n",
       " 'matter': 360,\n",
       " 'important': 361,\n",
       " 'ip': 362,\n",
       " '5': 363,\n",
       " 'saying': 364,\n",
       " 'times': 365,\n",
       " 'fucking': 366,\n",
       " 'review': 367,\n",
       " \"i'd\": 368,\n",
       " 'given': 369,\n",
       " 'website': 370,\n",
       " 'tell': 371,\n",
       " 'thought': 372,\n",
       " 'statement': 373,\n",
       " 'language': 374,\n",
       " 's': 375,\n",
       " '—': 376,\n",
       " 'bad': 377,\n",
       " 'title': 378,\n",
       " 'true': 379,\n",
       " '4': 380,\n",
       " 'once': 381,\n",
       " 'each': 382,\n",
       " 'real': 383,\n",
       " 'lot': 384,\n",
       " 'several': 385,\n",
       " 'term': 386,\n",
       " 'quite': 387,\n",
       " 'old': 388,\n",
       " 'instead': 389,\n",
       " 'claim': 390,\n",
       " 'until': 391,\n",
       " 'number': 392,\n",
       " 'revert': 393,\n",
       " 'maybe': 394,\n",
       " 'research': 395,\n",
       " '8': 396,\n",
       " 'facts': 397,\n",
       " 'mention': 398,\n",
       " 'year': 399,\n",
       " 'media': 400,\n",
       " 'always': 401,\n",
       " 'whether': 402,\n",
       " 'three': 403,\n",
       " 'template': 404,\n",
       " 'notable': 405,\n",
       " 'idea': 406,\n",
       " 'left': 407,\n",
       " 'changes': 408,\n",
       " 'correct': 409,\n",
       " 'consider': 410,\n",
       " 'following': 411,\n",
       " 'criteria': 412,\n",
       " 'useless': 413,\n",
       " 'd': 414,\n",
       " 'words': 415,\n",
       " 'pov': 416,\n",
       " 'means': 417,\n",
       " 'considered': 418,\n",
       " 'seem': 419,\n",
       " 'main': 420,\n",
       " 'fan': 421,\n",
       " \"there's\": 422,\n",
       " 'notice': 423,\n",
       " 'call': 424,\n",
       " 'c': 425,\n",
       " 'cannot': 426,\n",
       " 'admin': 427,\n",
       " 'american': 428,\n",
       " 'school': 429,\n",
       " 'homeland': 430,\n",
       " 'address': 431,\n",
       " 'second': 432,\n",
       " 'general': 433,\n",
       " '•': 434,\n",
       " 'course': 435,\n",
       " 'possible': 436,\n",
       " 'date': 437,\n",
       " 'guidelines': 438,\n",
       " 'makes': 439,\n",
       " 'current': 440,\n",
       " 'include': 441,\n",
       " 'getting': 442,\n",
       " 'securityfuck': 443,\n",
       " 'tommy2010': 444,\n",
       " 'including': 445,\n",
       " 'suggest': 446,\n",
       " 'group': 447,\n",
       " 'cool': 448,\n",
       " 'regarding': 449,\n",
       " 'man': 450,\n",
       " 'attack': 451,\n",
       " 'interest': 452,\n",
       " 'issues': 453,\n",
       " 'looking': 454,\n",
       " 'specific': 455,\n",
       " 'less': 456,\n",
       " 'end': 457,\n",
       " 'rules': 458,\n",
       " 'seen': 459,\n",
       " 'recent': 460,\n",
       " 'explain': 461,\n",
       " 'mentioned': 462,\n",
       " 'dont': 463,\n",
       " 'later': 464,\n",
       " 'whole': 465,\n",
       " 'line': 466,\n",
       " 'bob': 467,\n",
       " 'answer': 468,\n",
       " 'relevant': 469,\n",
       " 'sense': 470,\n",
       " 'ok': 471,\n",
       " 'days': 472,\n",
       " 'move': 473,\n",
       " 'kind': 474,\n",
       " 'although': 475,\n",
       " 'known': 476,\n",
       " 'project': 477,\n",
       " 'cunt': 478,\n",
       " 'changed': 479,\n",
       " 'care': 480,\n",
       " 'jpg': 481,\n",
       " 'start': 482,\n",
       " 'create': 483,\n",
       " 'org': 484,\n",
       " 'claims': 485,\n",
       " 'happy': 486,\n",
       " 'myself': 487,\n",
       " 'anyway': 488,\n",
       " '10': 489,\n",
       " 'p': 490,\n",
       " 'news': 491,\n",
       " 'listed': 492,\n",
       " 'info': 493,\n",
       " 'provide': 494,\n",
       " 'currently': 495,\n",
       " 'topic': 496,\n",
       " 'next': 497,\n",
       " 'atheist': 498,\n",
       " 'taken': 499,\n",
       " 'single': 500,\n",
       " 'anti': 501,\n",
       " 'stuff': 502,\n",
       " \"wikipedia's\": 503,\n",
       " 't': 504,\n",
       " \"you've\": 505,\n",
       " 'big': 506,\n",
       " 'certainly': 507,\n",
       " 'full': 508,\n",
       " 'summary': 509,\n",
       " '2005': 510,\n",
       " 'party': 511,\n",
       " 'books': 512,\n",
       " 'redirect': 513,\n",
       " 'attacks': 514,\n",
       " 'per': 515,\n",
       " 'four': 516,\n",
       " 'related': 517,\n",
       " 'category': 518,\n",
       " 'sentence': 519,\n",
       " '1967': 520,\n",
       " 'saget': 521,\n",
       " 'warning': 522,\n",
       " 'political': 523,\n",
       " 'talking': 524,\n",
       " 'file': 525,\n",
       " 'interested': 526,\n",
       " 'common': 527,\n",
       " 'wish': 528,\n",
       " 'picture': 529,\n",
       " 'knowledge': 530,\n",
       " 'country': 531,\n",
       " 'according': 532,\n",
       " 'self': 533,\n",
       " 'order': 534,\n",
       " 'public': 535,\n",
       " 'nor': 536,\n",
       " 'appears': 537,\n",
       " 'mind': 538,\n",
       " 'writing': 539,\n",
       " 'able': 540,\n",
       " 'love': 541,\n",
       " 'hey': 542,\n",
       " 'truth': 543,\n",
       " 'notability': 544,\n",
       " 'lead': 545,\n",
       " 'working': 546,\n",
       " 'licker': 547,\n",
       " 'faith': 548,\n",
       " 'especially': 549,\n",
       " 'names': 550,\n",
       " 'completely': 551,\n",
       " 'music': 552,\n",
       " 'web': 553,\n",
       " '20': 554,\n",
       " 'appropriate': 555,\n",
       " 'god': 556,\n",
       " 'discuss': 557,\n",
       " 'started': 558,\n",
       " 'position': 559,\n",
       " 'future': 560,\n",
       " 'published': 561,\n",
       " 'looks': 562,\n",
       " '6': 563,\n",
       " 'city': 564,\n",
       " 'major': 565,\n",
       " 'proposed': 566,\n",
       " 'included': 567,\n",
       " 'ago': 568,\n",
       " 'within': 569,\n",
       " 'official': 570,\n",
       " 'during': 571,\n",
       " 'response': 572,\n",
       " 'away': 573,\n",
       " 'neutral': 574,\n",
       " 'everything': 575,\n",
       " 'sign': 576,\n",
       " 'en': 577,\n",
       " '2007': 578,\n",
       " 'wanted': 579,\n",
       " 'nice': 580,\n",
       " 'removing': 581,\n",
       " 'wrote': 582,\n",
       " 'united': 583,\n",
       " 'false': 584,\n",
       " 'quote': 585,\n",
       " 'conflict': 586,\n",
       " 'community': 587,\n",
       " 'similar': 588,\n",
       " 'report': 589,\n",
       " 'form': 590,\n",
       " 'learn': 591,\n",
       " 'b': 592,\n",
       " 'reading': 593,\n",
       " '0': 594,\n",
       " 'asked': 595,\n",
       " 'rule': 596,\n",
       " 'policies': 597,\n",
       " 'game': 598,\n",
       " 'therefore': 599,\n",
       " '11': 600,\n",
       " '2006': 601,\n",
       " '9': 602,\n",
       " 'username': 603,\n",
       " 'unless': 604,\n",
       " 'npov': 605,\n",
       " 'today': 606,\n",
       " 'whatever': 607,\n",
       " 'side': 608,\n",
       " 'cheers': 609,\n",
       " 'pretty': 610,\n",
       " 'government': 611,\n",
       " 'live': 612,\n",
       " 'system': 613,\n",
       " 'tried': 614,\n",
       " 'deleting': 615,\n",
       " 'itself': 616,\n",
       " '24': 617,\n",
       " 'placed': 618,\n",
       " 'hard': 619,\n",
       " \"wasn't\": 620,\n",
       " 'came': 621,\n",
       " 'theory': 622,\n",
       " 'dispute': 623,\n",
       " '2008': 624,\n",
       " 'sourced': 625,\n",
       " 'penis': 626,\n",
       " 'paragraph': 627,\n",
       " 'reply': 628,\n",
       " 'present': 629,\n",
       " 'process': 630,\n",
       " 'obviously': 631,\n",
       " 'posted': 632,\n",
       " 'involved': 633,\n",
       " '7': 634,\n",
       " 'high': 635,\n",
       " '2010': 636,\n",
       " 'google': 637,\n",
       " \"won't\": 638,\n",
       " 'past': 639,\n",
       " 'stay': 640,\n",
       " 'reverting': 641,\n",
       " 'taking': 642,\n",
       " 'mud': 643,\n",
       " 'useful': 644,\n",
       " 'explanation': 645,\n",
       " 'edited': 646,\n",
       " 'play': 647,\n",
       " 'sort': 648,\n",
       " 'reasons': 649,\n",
       " 'below': 650,\n",
       " 'contribs': 651,\n",
       " 'open': 652,\n",
       " 'actual': 653,\n",
       " 'certain': 654,\n",
       " 'improve': 655,\n",
       " 'due': 656,\n",
       " 'views': 657,\n",
       " 'exactly': 658,\n",
       " 'short': 659,\n",
       " 'everyone': 660,\n",
       " 'needed': 661,\n",
       " '12': 662,\n",
       " '100': 663,\n",
       " 'guess': 664,\n",
       " 'along': 665,\n",
       " 'email': 666,\n",
       " 'sandbox': 667,\n",
       " 'become': 668,\n",
       " 'almost': 669,\n",
       " 'murder': 670,\n",
       " 'entire': 671,\n",
       " 'company': 672,\n",
       " 'problems': 673,\n",
       " 'interesting': 674,\n",
       " 'remember': 675,\n",
       " 'various': 676,\n",
       " 'de': 677,\n",
       " 'obvious': 678,\n",
       " 'band': 679,\n",
       " 'banned': 680,\n",
       " 'appreciate': 681,\n",
       " 'noticed': 682,\n",
       " 'admins': 683,\n",
       " 'fun': 684,\n",
       " 'dick': 685,\n",
       " 'internet': 686,\n",
       " 'w': 687,\n",
       " 'stated': 688,\n",
       " 'citation': 689,\n",
       " 'guy': 690,\n",
       " 'otherwise': 691,\n",
       " 'week': 692,\n",
       " 'entry': 693,\n",
       " 'status': 694,\n",
       " 'small': 695,\n",
       " 'cited': 696,\n",
       " 'argument': 697,\n",
       " 'search': 698,\n",
       " 'canada': 699,\n",
       " '15': 700,\n",
       " 'definition': 701,\n",
       " 'statements': 702,\n",
       " 'took': 703,\n",
       " 'alone': 704,\n",
       " 'went': 705,\n",
       " 'story': 706,\n",
       " 'law': 707,\n",
       " 'terms': 708,\n",
       " 'works': 709,\n",
       " 'follow': 710,\n",
       " 'themselves': 711,\n",
       " 'provided': 712,\n",
       " '\\xa0': 713,\n",
       " \"haven't\": 714,\n",
       " 'often': 715,\n",
       " 'color': 716,\n",
       " 'points': 717,\n",
       " 'kill': 718,\n",
       " 'likely': 719,\n",
       " 'death': 720,\n",
       " 'type': 721,\n",
       " 'stupid': 722,\n",
       " 'shit': 723,\n",
       " 'yeah': 724,\n",
       " '2004': 725,\n",
       " 'saw': 726,\n",
       " 'science': 727,\n",
       " 'cite': 728,\n",
       " 'soon': 729,\n",
       " 'g': 730,\n",
       " 'addition': 731,\n",
       " 'faggot': 732,\n",
       " 'proper': 733,\n",
       " 'piece': 734,\n",
       " 'series': 735,\n",
       " 'set': 736,\n",
       " 'national': 737,\n",
       " 'mr': 738,\n",
       " 'third': 739,\n",
       " 'king': 740,\n",
       " 'accurate': 741,\n",
       " 'uploaded': 742,\n",
       " 'indeed': 743,\n",
       " 'actions': 744,\n",
       " 'british': 745,\n",
       " 'shows': 746,\n",
       " 'disagree': 747,\n",
       " 'generally': 748,\n",
       " 'vandalize': 749,\n",
       " 'allowed': 750,\n",
       " 'power': 751,\n",
       " 'five': 752,\n",
       " 'calling': 753,\n",
       " 'rights': 754,\n",
       " 'regards': 755,\n",
       " 'told': 756,\n",
       " 'area': 757,\n",
       " 'human': 758,\n",
       " 'particular': 759,\n",
       " 'violation': 760,\n",
       " 'german': 761,\n",
       " \"wouldn't\": 762,\n",
       " 'm': 763,\n",
       " 'appear': 764,\n",
       " 'thus': 765,\n",
       " 'film': 766,\n",
       " 'tv': 767,\n",
       " 'accept': 768,\n",
       " 'himself': 769,\n",
       " 'external': 770,\n",
       " 'contributing': 771,\n",
       " 'assume': 772,\n",
       " 'background': 773,\n",
       " 'university': 774,\n",
       " 'india': 775,\n",
       " 'fine': 776,\n",
       " 'moved': 777,\n",
       " 'hours': 778,\n",
       " 'details': 779,\n",
       " '–': 780,\n",
       " 'black': 781,\n",
       " 'valid': 782,\n",
       " 'exist': 783,\n",
       " 'test': 784,\n",
       " 'guys': 785,\n",
       " 'cause': 786,\n",
       " 'class': 787,\n",
       " 'wikiproject': 788,\n",
       " 'attention': 789,\n",
       " 'nonsense': 790,\n",
       " 'gay': 791,\n",
       " 'legal': 792,\n",
       " 'contact': 793,\n",
       " 'administrator': 794,\n",
       " 'description': 795,\n",
       " '16': 796,\n",
       " 'together': 797,\n",
       " 'upon': 798,\n",
       " 'avoid': 799,\n",
       " 'culture': 800,\n",
       " 'outside': 801,\n",
       " 'helpful': 802,\n",
       " 'hand': 803,\n",
       " 'loser': 804,\n",
       " 'job': 805,\n",
       " 'goes': 806,\n",
       " 'attempt': 807,\n",
       " 'members': 808,\n",
       " 'debate': 809,\n",
       " \"let's\": 810,\n",
       " '19': 811,\n",
       " 'v': 812,\n",
       " \"he's\": 813,\n",
       " 'greek': 814,\n",
       " 'automatically': 815,\n",
       " 'seriously': 816,\n",
       " 'head': 817,\n",
       " \"aren't\": 818,\n",
       " \"article's\": 819,\n",
       " '000': 820,\n",
       " 'aware': 821,\n",
       " 'computer': 822,\n",
       " '2009': 823,\n",
       " 'period': 824,\n",
       " 'respect': 825,\n",
       " 'multiple': 826,\n",
       " 'lack': 827,\n",
       " 'ones': 828,\n",
       " 'bias': 829,\n",
       " 'recently': 830,\n",
       " 'result': 831,\n",
       " 'personally': 832,\n",
       " '23': 833,\n",
       " 'living': 834,\n",
       " 'citations': 835,\n",
       " 'r': 836,\n",
       " 'unblock': 837,\n",
       " 'level': 838,\n",
       " 'sites': 839,\n",
       " 'july': 840,\n",
       " '22': 841,\n",
       " 'french': 842,\n",
       " 'barnstar': 843,\n",
       " 'worked': 844,\n",
       " 'white': 845,\n",
       " '14': 846,\n",
       " 'south': 847,\n",
       " 'im': 848,\n",
       " 'context': 849,\n",
       " 'family': 850,\n",
       " 'previous': 851,\n",
       " 'online': 852,\n",
       " 'john': 853,\n",
       " 'enjoy': 854,\n",
       " 'meant': 855,\n",
       " 'happened': 856,\n",
       " 'serious': 857,\n",
       " 'comes': 858,\n",
       " 'north': 859,\n",
       " 'august': 860,\n",
       " 'data': 861,\n",
       " 'entirely': 862,\n",
       " 'incorrect': 863,\n",
       " 'tags': 864,\n",
       " 'asking': 865,\n",
       " 'complete': 866,\n",
       " 'gets': 867,\n",
       " \"what's\": 868,\n",
       " 'criticism': 869,\n",
       " '18': 870,\n",
       " 'modern': 871,\n",
       " 'watching': 872,\n",
       " 'team': 873,\n",
       " 'historical': 874,\n",
       " 'uk': 875,\n",
       " 'speak': 876,\n",
       " 'decide': 877,\n",
       " 'video': 878,\n",
       " 'earlier': 879,\n",
       " 'uses': 880,\n",
       " 'available': 881,\n",
       " 'sock': 882,\n",
       " 'author': 883,\n",
       " 'simple': 884,\n",
       " 'changing': 885,\n",
       " \"they're\": 886,\n",
       " 'children': 887,\n",
       " 'creating': 888,\n",
       " 'blocking': 889,\n",
       " 'f': 890,\n",
       " 'close': 891,\n",
       " 'specifically': 892,\n",
       " 'standard': 893,\n",
       " '30': 894,\n",
       " 'wikipedian': 895,\n",
       " 'months': 896,\n",
       " 'refer': 897,\n",
       " 'heard': 898,\n",
       " 'al': 899,\n",
       " 'run': 900,\n",
       " 'doubt': 901,\n",
       " 'accepted': 902,\n",
       " 'copy': 903,\n",
       " 'manual': 904,\n",
       " 'supposed': 905,\n",
       " 'apparently': 906,\n",
       " 'jewish': 907,\n",
       " 'none': 908,\n",
       " 'album': 909,\n",
       " 'arguments': 910,\n",
       " 'explaining': 911,\n",
       " 'ban': 912,\n",
       " 'missing': 913,\n",
       " 'light': 914,\n",
       " 'fail': 915,\n",
       " 'biased': 916,\n",
       " 'financial': 917,\n",
       " 'figure': 918,\n",
       " 'study': 919,\n",
       " 'press': 920,\n",
       " 'bastard': 921,\n",
       " 'face': 922,\n",
       " 'understanding': 923,\n",
       " 'sometimes': 924,\n",
       " \"you'll\": 925,\n",
       " 'html': 926,\n",
       " 'tagged': 927,\n",
       " 'coming': 928,\n",
       " 'events': 929,\n",
       " 'deal': 930,\n",
       " 'putting': 931,\n",
       " \"shouldn't\": 932,\n",
       " 'action': 933,\n",
       " '13': 934,\n",
       " 'december': 935,\n",
       " 'special': 936,\n",
       " 'release': 937,\n",
       " 'homo': 938,\n",
       " 'international': 939,\n",
       " 'sections': 940,\n",
       " 'shall': 941,\n",
       " 'o': 942,\n",
       " 'messages': 943,\n",
       " 'meaning': 944,\n",
       " 'longer': 945,\n",
       " 'contest': 946,\n",
       " 'rationale': 947,\n",
       " 'cities': 948,\n",
       " 'necessary': 949,\n",
       " 'member': 950,\n",
       " 'america': 951,\n",
       " 'warring': 952,\n",
       " 'act': 953,\n",
       " 'photo': 954,\n",
       " 'academic': 955,\n",
       " 'shut': 956,\n",
       " 'fucker': 957,\n",
       " 'proof': 958,\n",
       " 'large': 959,\n",
       " 'prominent': 960,\n",
       " 'purpose': 961,\n",
       " 'rest': 962,\n",
       " 'dead': 963,\n",
       " 'march': 964,\n",
       " 'fix': 965,\n",
       " 'business': 966,\n",
       " 'border': 967,\n",
       " 'vote': 968,\n",
       " 'christian': 969,\n",
       " 'allow': 970,\n",
       " 'tildes': 971,\n",
       " 'except': 972,\n",
       " 'x': 973,\n",
       " '21': 974,\n",
       " 'expect': 975,\n",
       " 'western': 976,\n",
       " 'century': 977,\n",
       " 'force': 978,\n",
       " 'population': 979,\n",
       " 'factual': 980,\n",
       " 'june': 981,\n",
       " 'hell': 982,\n",
       " 'readers': 983,\n",
       " 'pro': 984,\n",
       " 'civil': 985,\n",
       " 'home': 986,\n",
       " \"'\": 987,\n",
       " '25': 988,\n",
       " 'lol': 989,\n",
       " 'age': 990,\n",
       " 'born': 991,\n",
       " 'nobody': 992,\n",
       " 'space': 993,\n",
       " 'minor': 994,\n",
       " 'lists': 995,\n",
       " 'situation': 996,\n",
       " '17': 997,\n",
       " 'bring': 998,\n",
       " 'afd': 999,\n",
       " 'wants': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = token.word_index\n",
    "word_index # getting the dict of vocabulaory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1500, 300)         13049100  \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 100)               40100     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,089,301\n",
      "Trainable params: 13,089,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: total: 828 ms\n",
      "Wall time: 738 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # A simpleRNN without any pretrained embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     input_length=max_len))\n",
    "    model.add(SimpleRNN(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Sequential(): This creates a sequential model, which is a linear stack of layers.\n",
    "\n",
    "model.add(Embedding(len(word_index) + 1, 300, input_length=max_len)): This adds an Embedding layer to the model. The embedding layer is used to convert integer-encoded words (represented by indices) into dense vectors of fixed size. It's common in NLP tasks to use pre-trained word embeddings, but in this case, it seems to be learning embeddings from scratch.\n",
    "\n",
    "len(word_index) + 1 is the input dimension, representing the size of the vocabulary.\n",
    "300 is the size of the dense embedding for each word.\n",
    "input_length=max_len specifies the maximum length of the input sequences.\n",
    "so, considering a sentence, then at this stage, each word will have vector dimension of 300 and  max_len i.e 1500 words can be present in one sentence, more than that will be truncated below that will be padded.\n",
    "\n",
    "model.add(SimpleRNN(100)): This adds a Simple RNN (Recurrent Neural Network) layer with 100 units to the model. RNNs are capable of processing sequences of data.\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid')): This adds a dense layer with a single unit and a sigmoid activation function. The output is binary, and sigmoid is commonly used for binary classification problems.\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']): This compiles the model. It specifies the loss function (binary_crossentropy for binary classification), the optimizer (adam), and the evaluation metric (accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "150/150 [==============================] - 457s 3s/step - loss: 0.3193 - accuracy: 0.8955\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 1130s 8s/step - loss: 0.3683 - accuracy: 0.8877\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 1702s 11s/step - loss: 0.1960 - accuracy: 0.9494\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 1703s 11s/step - loss: 0.1430 - accuracy: 0.9583\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 893s 6s/step - loss: 0.1195 - accuracy: 0.9614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18988f46b50>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad,ytrain,epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 75s 980ms/step\n",
      "Auc: 0.73%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model = []\n",
    "scores_model.append({'Model': 'SimpleRNN','AUC_Score': roc_auc(scores,yvalid)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanantion\n",
    "* Tokenization<br><br>\n",
    " So if you have watched the videos and referred to the links, you would know that in an RNN we input a sentence word by word. We represent every word as one hot vectors of dimensions : Numbers of words in Vocab +1. <br>\n",
    "  What keras Tokenizer does is , it takes all the unique words in the corpus,forms a dictionary with words as keys and their number of occurences as values,it then sorts the dictionary in descending order of counts. It then assigns the first value 1 , second value 2 and so on. So let's suppose word 'the' occured the most in the corpus then it will assigned index 1 and vector representing 'the' would be a one-hot vector with value 1 at position 1 and rest zereos.<br>\n",
    "  Try printing first 2 elements of xtrain_seq you will see every word is represented as a digit now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[664,\n",
       "   65,\n",
       "   7,\n",
       "   19,\n",
       "   2262,\n",
       "   14102,\n",
       "   5,\n",
       "   2262,\n",
       "   20439,\n",
       "   6071,\n",
       "   4,\n",
       "   71,\n",
       "   32,\n",
       "   20440,\n",
       "   6620,\n",
       "   39,\n",
       "   6,\n",
       "   664,\n",
       "   65,\n",
       "   11,\n",
       "   8,\n",
       "   20441,\n",
       "   1502,\n",
       "   38,\n",
       "   6072]],\n",
       " 25)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_seq[:1], len(xtrain_seq[:1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "While building our simple RNN models we talked about using word-embeddings , So what is word-embeddings and how do we get word-embeddings?\n",
    "Here is the answer :\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/6Oq70/word-representation\n",
    "* https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "<br> <br>\n",
    "The latest approach to getting word Embeddings is using pretained GLoVe or using Fasttext. Without going into too much details, I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it and since I am a fan of GloVe vectors, word2vec and fasttext. In this Notebook, I'll be using the GloVe vectors. You can download the GloVe vectors from here http://www-nlp.stanford.edu/data/glove.840B.300d.zip or you can search for GloVe in datasets on Kaggle and add the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:21, 18792.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('D:\\kiran\\RNN\\glove.6B.100d.txt','r',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray([float(val) for val in values[1:]])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM's\n",
    "\n",
    "## Basic Overview\n",
    "\n",
    "Simple RNN's were certainly better than classical ML algorithms and gave state of the art results, but it failed to capture long term dependencies that is present in sentences . So in 1998-99 LSTM's were introduced to counter to these drawbacks.\n",
    "\n",
    "## In Depth Understanding\n",
    "\n",
    "Why LSTM's?\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/PKMRR/vanishing-gradients-with-rnns\n",
    "* https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/\n",
    "\n",
    "What are LSTM's?\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/KXoay/long-short-term-memory-lstm\n",
    "* https://distill.pub/2019/memorization-in-rnns/\n",
    "* https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "# Code Implementation\n",
    "\n",
    "We have already tokenized and paded our text for input to LSTM's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43496 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43496/43496 [00:00<00:00, 309782.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1500, 100)         4349700   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,430,201\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 4,349,700\n",
      "_________________________________________________________________\n",
      "CPU times: total: 594 ms\n",
      "Wall time: 400 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    \n",
    "    # A simple LSTM with glove embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     100,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "\n",
    "    model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "150/150 [==============================] - 18101s 121s/step - loss: 0.2505 - accuracy: 0.9108\n",
      "Epoch 2/2\n",
      "150/150 [==============================] - 15516s 103s/step - loss: 0.1812 - accuracy: 0.9376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1899228ca90>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 56s 707ms/step\n",
      "Auc: 0.94%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model.append({'Model': 'LSTM','AUC_Score': roc_auc(scores,yvalid)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Model': 'SimpleRNN', 'AUC_Score': 0.7277841044948511},\n",
       " {'Model': 'LSTM', 'AUC_Score': 0.9443928850775484}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation\n",
    "\n",
    "As a first step we calculate embedding matrix for our vocabulary from the pretrained GLoVe vectors . Then while building the embedding layer we pass Embedding Matrix as weights to the layer instead of training it over Vocabulary and thus we pass trainable = False.\n",
    "Rest of the model is same as before except we have replaced the SimpleRNN By LSTM Units\n",
    "\n",
    "* Comments on the Model\n",
    "\n",
    "We now see that the model is not overfitting and achieves an auc score of 0.93 which is quite commendable , also we close in on the gap between accuracy and auc .\n",
    "We see that in this case we used dropout and prevented overfitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU's\n",
    "\n",
    "## Basic  Overview\n",
    "\n",
    "Introduced by Cho, et al. in 2014, GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU's are a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results . GRU's were designed to be simpler and faster than LSTM's and in most cases produce equally good results and thus there is no clear winner.\n",
    "\n",
    "## In Depth Explanation\n",
    "\n",
    "* https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/agZiL/gated-recurrent-unit-gru\n",
    "* https://www.geeksforgeeks.org/gated-recurrent-unit-networks/\n",
    "\n",
    "## Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 1500, 100)         4349700   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 1500, 100)        0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 300)               361800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 301       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,711,801\n",
      "Trainable params: 362,101\n",
      "Non-trainable params: 4,349,700\n",
      "_________________________________________________________________\n",
      "CPU times: total: 750 ms\n",
      "Wall time: 687 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # GRU with glove embeddings and two dense layers\n",
    "     model = Sequential()\n",
    "     model.add(Embedding(len(word_index) + 1,\n",
    "                     100,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "     model.add(SpatialDropout1D(0.3))\n",
    "     model.add(GRU(300))\n",
    "     model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "     model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])   \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 10898s 73s/step - loss: 0.2388 - accuracy: 0.9191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18a0612f790>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 104s 1s/step\n",
      "Auc: 0.94%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_model.append({'Model': 'GRU','AUC_Score': roc_auc(scores,yvalid)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Model': 'SimpleRNN', 'AUC_Score': 0.7277841044948511},\n",
       " {'Model': 'LSTM', 'AUC_Score': 0.9443928850775484},\n",
       " {'Model': 'GRU', 'AUC_Score': 0.9371851557655756}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Directional RNN's\n",
    "\n",
    "## In Depth Explanation\n",
    "\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/fyXnn/bidirectional-rnn\n",
    "* https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66\n",
    "* https://d2l.ai/chapter_recurrent-modern/bi-rnn.html\n",
    "\n",
    "## Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 1500, 100)         4349700   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 200)              160800    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,510,701\n",
      "Trainable params: 161,001\n",
      "Non-trainable params: 4,349,700\n",
      "_________________________________________________________________\n",
      "CPU times: total: 609 ms\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # A simple bidirectional LSTM with glove embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     100,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "    model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))\n",
    "\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/150 [>.............................] - ETA: 47:53:06 - loss: 0.4574 - accuracy: 0.8906"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtrain_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\envs\\RNN_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\kiran.franklin\\Anaconda3\\envs\\RNN_env\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RNN_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
